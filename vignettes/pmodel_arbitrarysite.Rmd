---
title: "P-model at an arbitrary site"
author: "Beni Stocker"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{P-model at an arbitrary site}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

## Aim

This vignette demonstrates the use of {rsofun} for an ensemble of arbitrary points on the globe, given their longitude and latitude coordinates. Model forcing data is constructed by reading data from global climate data. This is in contrast to using measured time series of climatic variables obtained from a site network (as described in [this article](./vignettes/pmodel_fluxnet.Rmd).

The R package {ingestr} is used for collecting point-level forcing time series. Install it directly from GitHub (see code chunk at the top of this article).

In this vignette, we use the paths corresponding to the GECO data archive (more information [here](https://github.com/geco-bern/data_management)). Note that this code reads files from their local paths. These files are not contained by the *rsofundemo* repository.

Finally, we format the data to run the [rsofun](https://geco-bern.github.io/rsofun/) P-model implementation.

## Libraries

First, load required libraries available on CRAN.
```{r message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(here)
library(visdat)
```

Then, install and load required libraries that are not on CRAN. Instead, obtain them directly from GitHub.
```{r}
if(!require(remotes)){install.packages(remotes)}
remotes::install_github("geco-bern/rsofun")
remotes::install_github("geco-bern/ingestr")
library(rsofun)
library(ingestr)
```


## Collect data

### Site meta information

The starting point is a data frame of site meta information with site names and geographic locations in degrees east and north. Also required is the starting and end year of the simulation by site. Let's use the same start and end year for all sites. This data frame needs to be specified by the user.
```{r}
siteinfo <- tibble(
  sitename = c("site1", "site2", "site3"), # AU-How, CH-Dav, US-Ha1
  lon = c(131.149994, 9.855910, -72.171500),
  lat = c(-12.495200, 46.815330, 42.53780)
) |> 
  mutate(
    year_start = 2010,
    year_end = 2012
  )
```

### Meteorological forcing

The following meteorological variables are obtained for the P-model forcing
from the [WATCH-WFDEI](https://rda.ucar.edu/datasets/ds314.2/) data for the following variables in {rsofun} standard naming:

- `temp`: Daily temperature
- `prec`: Daily precipitation
- `ppfd`: Photosynthetic photon flux density
- `vpd`: Vapor pressure deficit
- `patm`: Atmospheric pressure

WATCH-WFDEI data is provided at a spatial resolution of 0.5 degrees. To minimise errors related to small-scale climatic gradients related to topography, we use a simple downscaling based on the WorldClim high resolution (1/120 degrees globally) monthly climatology. The downscaling is based on removing the bias by month (for temperature, PPFD, atmospheric pressure) or re-scaling to avoid bias (for precipitation and VPD). This is implemented in `ingestr::ingest()`.

Loading this data can take a while, up to hours if you need several years of data and for many sites.

```{r, eval=FALSE, message=FALSE, warning=FALSE}
# local paths on GECO server
path_watch <- "/data/archive/wfdei_weedon_2014/data"
path_worldclim <- "/data/archive/worldclim_fick_2017/data"

df_watch <- ingestr::ingest(
  siteinfo = siteinfo,
  source = "watch_wfdei",
  getvars = c("temp", "prec", "ppfd", "vpd", "patm"),
  dir = path_watch,
  settings = list(
    correct_bias = "worldclim",
    dir_bias = path_worldclim
  )
)
```

### Cloud cover from CRU

Now, let's complete the forcing with cloud cover `ccov` values from 
[CRU](https://crudata.uea.ac.uk/cru/data/hrg/) data. 

```{r, eval=FALSE}
# Get CRU data
path_cru <- "/data/archive/cru_NA_2021/data/"

df_cru <- ingestr::ingest(
  siteinfo = siteinfo,
  source = "cru",
  getvars = c("ccov"),
  dir = path_cru,
  settings = list(
    correct_bias = NULL       # 0.5 deg resolution
  )
)
```

### Merge meteorological drivers

Let's put together the previous data, into a single data.frame:
```{r, eval=FALSE}
df_meteo <- df_watch |>
  tidyr::unnest(data) |>
  left_join(
    df_cru |>
      tidyr::unnest(data),
    by = c("sitename", "date")
  ) |>
  dplyr::ungroup() |>                         # keep unnested
  dplyr::mutate(tmin = temp,                  # placeholders for variables tmin and tmax
                tmax = temp) |>               # (not used in P-model runs)
  dplyr::mutate(doy = lubridate::yday(date))  # add day of year

head(df_meteo)
```

### Get CO2 data

The following chunk downloads the CO2 yearly average data from the Mauna Loa 
observatory and appends it to the meteorological drivers from above.
```{r, eval=FALSE}
# Download CO2 data
df_co2 <- read.csv(
  url("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv"),
  skip = 59) |>
  dplyr::select(year, mean) |>
  dplyr::rename(co2 = mean)

# Merge with meteo drivers
df_meteo <- df_meteo |>
  dplyr::mutate(year = lubridate::year(date)) |>
  dplyr::left_join(df_co2, by = "year")            # keep unnested
```

### Append fAPAR

Set fAPAR by default to 1 for all observations, which is the same as what
`ingest(siiteinfo_globresp, source = "fapar_unity")` would do. This makes
sense for leaf-level simulations.
```{r, eval=FALSE}
# Add fapar column with value 1
df_meteo <- df_meteo |>
  dplyr::mutate(fapar = 1)
```

## Put all data together into rsofun object

Up to here, we have merged all of the forcing data into `df_meteo`. Let's
put it in a nested format, such that there is a single data.frame per site:
```{r, eval=FALSE}
# Nest forcing
df_meteo <- df_meteo |>
  dplyr::group_by(sitename) |>
  tidyr::nest() |>
  dplyr::rename(forcing = data) |>
  dplyr::ungroup()

# Add site information
df_siteinfo <- siteinfo |>
  dplyr::group_by(sitename) |>
  tidyr::nest() |>
  dplyr::rename(site_info = data) |>
  dplyr::ungroup()
```

